{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5792e-04, -7.1352e-01,  2.3110e-01,  9.1366e-01,  9.0954e-02],\n",
      "        [-5.3670e-01, -5.7177e-01, -5.1318e-01,  2.3451e-01, -1.5078e+00],\n",
      "        [-2.2963e-01, -3.7097e-01, -1.6495e-02,  3.2753e-01, -1.7171e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\n\\nA linear layer is created which maps input tensors from a 10-dimensional space to a 5-dimensional space.\\nA random input tensor of size [3, 10] (indicating a batch of 3 samples, each with 10 features) is passed through the layer.\\nThe output will be a tensor of size [3, 5], where each of the 3 samples now has 5 features.\\n\\nSuppose you have a nn.Linear layer that outputs 5 features, and you want to connect this output to another nn.Linear layer that requires 5 input features. \\nYou would set up the first layer as nn.Linear(in_features=10, out_features=5) and the subsequent layer might be nn.Linear(in_features=5, out_features=<some other number>).\\nThis chaining is typical in neural networks where each layer's output becomes the next layer's input, enabling complex transformations and deep learning architectures.\\n\\nIf you want the linear transformation to serve as the final operation for a particular functionality, such as outputting a single value for regression or binary classification, \\nyou could set it to have only 1 output feature. For example, if your model ends with a prediction, such as predicting a price or classifying between two categories, \\nyou would use nn.Linear(in_features=10, out_features=1).\\nThis setup means the layer will transform all the incoming features into a single scalar value, which might represent a predicted score, probability, or some other targeted output.\\n\\n\\nConstructor of nn.Linear class: \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "The primary module used in pytorch is the \"torch.nn\" module. The commonly used classes and their important methods are below : \n",
    "- nn.Linear: \n",
    "he nn.Linear class in PyTorch represents a linear or fully connected layer, which is a basic building block in many neural networks. It is particularly common in the architectures of traditional neural networks, as well as deep learning networks used for tasks like regression, classification, and as components in more complex models like convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n",
    "\n",
    "Functionality\n",
    "The primary function of the nn.Linear layer is to apply a linear transformation to the incoming data. This transformation is mathematically represented as:\n",
    "\n",
    "y=xA.T +b\n",
    "\n",
    "Where:\n",
    "\n",
    "ùë•\n",
    "x is the input tensor,\n",
    "ùê¥\n",
    "A represents the weights matrix,\n",
    "ùëè\n",
    "b is the bias vector,\n",
    "ùë¶\n",
    "y is the output tensor.\n",
    "Parameters\n",
    "When you instantiate an nn.Linear module, you need to specify two important parameters:\n",
    "\n",
    "in_features: The size of each input sample. This is the number of features in the input data.\n",
    "out_features: The size of each output sample. This is the number of features in the output data.\n",
    "Optionally, you can specify a third parameter:\n",
    "\n",
    "bias: A Boolean value (True by default) that specifies whether to add the bias term \n",
    "ùëè\n",
    "b in the linear equation. If set to False, the layer will not learn an additive bias.\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the linear layer\n",
    "linear_layer = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# Example input tensor (batch size of 3, each sample having 10 features)\n",
    "input_tensor = torch.randn(3, 10)\n",
    "\n",
    "# Forward pass through the linear layer\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "# Output the result\n",
    "print(output_tensor)\n",
    "\n",
    "''' \n",
    "\n",
    "A linear layer is created which maps input tensors from a 10-dimensional space to a 5-dimensional space.\n",
    "A random input tensor of size [3, 10] (indicating a batch of 3 samples, each with 10 features) is passed through the layer.\n",
    "The output will be a tensor of size [3, 5], where each of the 3 samples now has 5 features.\n",
    "\n",
    "Suppose you have a nn.Linear layer that outputs 5 features, and you want to connect this output to another nn.Linear layer that requires 5 input features. \n",
    "You would set up the first layer as nn.Linear(in_features=10, out_features=5) and the subsequent layer might be nn.Linear(in_features=5, out_features=<some other number>).\n",
    "This chaining is typical in neural networks where each layer's output becomes the next layer's input, enabling complex transformations and deep learning architectures.\n",
    "\n",
    "If you want the linear transformation to serve as the final operation for a particular functionality, such as outputting a single value for regression or binary classification, \n",
    "you could set it to have only 1 output feature. For example, if your model ends with a prediction, such as predicting a price or classifying between two categories, \n",
    "you would use nn.Linear(in_features=10, out_features=1).\n",
    "This setup means the layer will transform all the incoming features into a single scalar value, which might represent a predicted score, probability, or some other targeted output.\n",
    "\n",
    "\n",
    "Constructor of nn.Linear class:  '''\n",
    " __init__(self, in_features, out_features, bias=True)\n",
    "\n",
    "''' \n",
    "Purpose: Initializes a new nn.Linear instance with specified parameters.\n",
    "Parameters:\n",
    "in_features: The number of input features.\n",
    "out_features: The number of output features.\n",
    "bias: A boolean indicating whether a bias term should be added to the output (defaults to True).\n",
    "\n",
    "'''\n",
    "\n",
    "forward(self, input) : \n",
    "\n",
    "'''\n",
    "Purpose: Defines the computation performed at every call. For nn.Linear, it performs the linear transformation to the incoming data.\n",
    "Parameters:\n",
    "input: The input data tensor.\n",
    "Usage: This method is not usually called directly but is used by PyTorch when the module is called with its parameters (e.g., output = linear_layer(input)).\n",
    "Implementation: Typically involves multiplying the input by the layer's weight matrix and optionally adding the bias, \n",
    "\n",
    "In the context of using nn.Linear, the input tensor you pass to the layer during the forward pass should indeed be the feature matrix. \n",
    "\n",
    "Shape: If you're using a nn.Linear layer configured as nn.Linear(in_features, out_features), the input tensor should have a shape of [batch_size, in_features]. Here:\n",
    "batch_size is the number of examples you are processing in one forward pass (this can be any size, including 1).\n",
    "in_features must match the number of input features expected by the layer, which is the dimensionality of each example in your feature matrix.\n",
    "\n",
    "\n",
    "Parameter Access Methods : \n",
    "\n",
    "parameters()\n",
    "Purpose: Returns an iterator over module parameters (weights and biases for nn.Linear), which is often used for passing parameters to optimizers or for manual inspection.\n",
    "named_parameters()\n",
    "\n",
    "Purpose: Similar to parameters(), but returns both the name of the parameter as well as the parameter itself, useful for debugging or when you need to treat parameters \n",
    "differently depending on their roles.\n",
    "\n",
    "State Management Methods:\n",
    "\n",
    "state_dict()\n",
    "Purpose: Returns a dictionary containing a whole state of the module including its parameters and persistent buffers (e.g., running averages), \n",
    "which is used for saving or loading models from disk.\n",
    "\n",
    "load_state_dict(state_dict, strict=True)\n",
    "Purpose: Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys \n",
    "returned by this module‚Äôs state_dict() function.\n",
    "\n",
    "Device Management Methods:\n",
    "\n",
    "to(device)\n",
    "Purpose: Moves and/or casts the parameters and buffers to the specified device or data type. This is used to move a module from CPU to GPU, or vice versa, or \n",
    "to change the data type of the module parameters.\n",
    "\n",
    "cuda()\n",
    "Purpose: Moves all model parameters and buffers to the GPU.\n",
    "\n",
    "cpu()\n",
    "Purpose: Moves all model parameters and buffers back to the CPU.\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Activation function classes : \n",
    "\n",
    "nn.ReLU class: \n",
    "The nn.ReLU class in PyTorch is a module that applies the Rectified Linear Unit (ReLU) activation function. ReLU is one of the most commonly used activation functions in \n",
    "neural networks, especially in deep neural networks, because of its computational simplicity and effectiveness in mitigating the vanishing gradient problem. \n",
    "The ReLU function is defined as:\n",
    "f(x)=max(0,x)\n",
    "\n",
    "Constructor\n",
    "The constructor for the nn.ReLU class is straightforward and can be configured to perform the ReLU operation in an in-place manner, which can sometimes save memory:\n",
    "'''\n",
    "torch.nn.ReLU(inplace=False)\n",
    "'''\n",
    "Parameters:\n",
    "inplace (bool, optional): If set to True, it modifies the input data in place, which can save memory but might risk corrupting data during backpropagation if the input tensor \n",
    "is used elsewhere in your model. The default is False.\n",
    "\n",
    "'''\n",
    "\n",
    "forward(input)\n",
    "\n",
    "'''\n",
    "Purpose: Computes the ReLU function on the input tensor. This method is usually not called directly; instead, the module itself is called with the input tensor, and the forward method is invoked internally.\n",
    "Parameters:\n",
    "input (Tensor): The input tensor on which to apply the ReLU function.\n",
    "Returns: A tensor with the same shape as the input, where each element is the result of applying ReLU to the corresponding element of the input tensor.\n",
    "\n",
    "'''\n",
    "\n",
    "''' \n",
    "nn.Parameter() is a special type of tensor that tells the framework that this tensor should be considered a model parameter. \n",
    "This is especially useful for defining learnable parameters (like weights and biases) inside custom models that inherit from \n",
    "nn.Module.\n",
    "\n",
    "When you wrap a tensor with nn.Parameter(), PyTorch knows:\n",
    "\n",
    "This tensor should be included in the model's parameter list.\n",
    "Gradients will be tracked for it, meaning it will be updated during backpropagation using optimizers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Saving / Loading a Model:\n",
    "\n",
    "There are three neain methods you should know about for saving and loading models in PyTorch.\n",
    "\n",
    "1. `torch.save()` -  allows you to save a PyTorch object in Python's pickle format.\n",
    "2. `torch.load()` - allows you to load a saved PyTorch object\n",
    "3. `torch.nn.Module.load_state_dict()` - this allows you to load a model's saved state dictionary\n",
    "\n",
    "[About Loading and Saving Models in PyTorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "\n",
    "We can save and load the entire model that we create but the recommended way is to save the model state_dict.\n",
    "\n",
    "                                                                                                                                                                                                                        wh\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
