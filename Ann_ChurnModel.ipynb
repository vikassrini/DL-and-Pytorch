{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:111: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:111: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\vikas\\AppData\\Local\\Temp\\ipykernel_5216\\3133759160.py:111: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  data = pd.read_csv(\"datasets\\Churn_Modelling.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 11.51290950247485\n",
      "Epoch 100, Loss: 7.985148488530836\n",
      "Epoch 200, Loss: 7.600884249464613\n",
      "Epoch 300, Loss: 7.534952804790826\n",
      "Epoch 400, Loss: 7.523555144223419\n",
      "Epoch 500, Loss: 7.524869962464565\n",
      "Epoch 600, Loss: 7.5339944331266215\n",
      "Epoch 700, Loss: 7.544801606140152\n",
      "Epoch 800, Loss: 7.515470386276193\n",
      "Epoch 900, Loss: 7.422286084569029\n",
      "Train Accuracy: 79.45%\n",
      "Test Accuracy: 80.35%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to clean data\n",
    "def clean_data(data):\n",
    "    #data = data[data[\"Balance\"] != 0]\n",
    "    data = data.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1)\n",
    "    return data\n",
    "\n",
    "# Function to perform one-hot encoding\n",
    "def Categorical_To_Numerical(data, col):\n",
    "    uniques = data[col].unique()\n",
    "    unique_dict = {value: idx for idx, value in enumerate(uniques)}\n",
    "    data[col] = data[col].map(unique_dict)\n",
    "    return data\n",
    "\n",
    "# Function to scale features\n",
    "def scale_features(data, feature_columns):\n",
    "    scaler = MinMaxScaler()\n",
    "    data[feature_columns] = scaler.fit_transform(data[feature_columns])\n",
    "    return data\n",
    "\n",
    "# Initialize parameters\n",
    "def initialize_params():\n",
    "    np.random.seed(42)\n",
    "    params = {\n",
    "        'W(hn1)': np.random.randn(noofipnodes, noofhiddennodes) * 0.01,\n",
    "        'B(hn1)': np.zeros((1, noofhiddennodes)),\n",
    "        'W(op)': np.random.randn(noofhiddennodes, nofopnodes) * 0.01,\n",
    "        'B(op)': np.zeros((1, nofopnodes)),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_prop(X, params):\n",
    "    Z1 = np.dot(X, params[\"W(hn1)\"]) + params['B(hn1)']\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, params[\"W(op)\"]) + params['B(op)']\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = (Z1, A1, Z2, A2)\n",
    "    return A2, cache\n",
    "\n",
    "# Compute loss\n",
    "def compute_loss(A2, y):\n",
    "    m = y.shape[0]\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    A2 = np.clip(A2, epsilon, 1 - epsilon)  # Clipping the values\n",
    "    loss = -1/m * np.sum(y * np.log(A2) + (1 - y) * np.log(1 - A2))\n",
    "    return loss\n",
    "\n",
    "# Backward propagation\n",
    "def backward_prop(X, y, cache, params):\n",
    "    m = y.shape[0]  # Corrected this line\n",
    "    (Z1, A1, Z2, A2) = cache \n",
    "\n",
    "    # Gradient with respect to output layer weights and biases\n",
    "    dZ2 = A2 - y.reshape(-1, 1)\n",
    "    dW_op = np.dot(A1.T, dZ2) / m\n",
    "    dB_op = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Gradient with respect to hidden layer weights and biases\n",
    "    dA1 = np.dot(dZ2, params['W(op)'].T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW_hn1 = np.dot(X.T, dZ1) / m\n",
    "    dB_hn1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    grads = {\"HLW\": dW_hn1, \"HLB\": dB_hn1, \"OLW\": dW_op, \"OLB\": dB_op}\n",
    "    return grads\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    params['W(hn1)'] -= learning_rate * grads[\"HLW\"]\n",
    "    params['B(hn1)'] -= learning_rate * grads[\"HLB\"]\n",
    "    params['W(op)'] -= learning_rate * grads[\"OLW\"]\n",
    "    params['B(op)'] -= learning_rate * grads[\"OLB\"]\n",
    "    return params\n",
    "\n",
    "# Training function\n",
    "def train(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
    "    params = initialize_params()\n",
    "    for i in range(epochs):\n",
    "        A2, cache = forward_prop(X_train, params)\n",
    "        loss = compute_loss(y_train, A2)\n",
    "        grads = backward_prop(X_train, y_train, cache, params)\n",
    "        params = update_parameters(params, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}, Loss: {loss}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, params):\n",
    "    A2, _ = forward_prop(X, params)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"datasets\\Churn_Modelling.csv\")\n",
    "\n",
    "# Clean data\n",
    "\n",
    "cleaned_data=Categorical_To_Numerical(data,\"Gender\")\n",
    "cleaned_data=Categorical_To_Numerical(data,\"Geography\")\n",
    "cleaned_data = clean_data(data)\n",
    "\n",
    "continuous_features = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"]\n",
    "normalized_data = scale_features(cleaned_data, continuous_features)\n",
    "\n",
    "normalized_data.to_csv(\"Prepared Churn Model Data.csv\", index=False)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = normalized_data.drop(\"Exited\", axis=1).values\n",
    "y = normalized_data[\"Exited\"].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# NN architecture\n",
    "noofipnodes = X_train.shape[1]\n",
    "noofhiddennodes = 2\n",
    "nofopnodes = 1\n",
    "\n",
    "params = train(X_train, y_train, learning_rate=0.1, epochs=1000)\n",
    "y_pred_train = predict(X_train, params)\n",
    "y_pred_test = predict(X_test, params)\n",
    "\n",
    "train_accuracy = np.mean(y_pred_train == y_train.reshape(-1, 1))\n",
    "test_accuracy = np.mean(y_pred_test == y_test.reshape(-1, 1))\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy * 100}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
